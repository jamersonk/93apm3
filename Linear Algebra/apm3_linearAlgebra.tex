\documentclass[14pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{algpseudocode}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{color}
\usepackage{setspace}
\onehalfspacing
\usepackage{titlesec}
\usepackage{paralist}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\titleformat{\section}[block]{\color{black}\Large\bfseries\filcenter}{}{1em}{}

\geometry{
    a4paper,
    nomarginpar,
    includeheadfoot,
    total = {140mm, 257mm},
    headheight = 1.2cm
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
    }

    \theoremstyle{definition}
    \newtheorem*{remark}{Remarks}
    \newtheorem*{example}{Example}
    \newtheorem*{discussion}{Discussion}
    \newtheorem{definition}{Definition}[subsection]
    \newtheorem{proposition}[definition]{Proposition}
    \newtheorem{theorem}[definition]{Theorem}
    \newtheorem{notation}[definition]{Notation}
    \newtheorem{coro}[definition]{Corollary}
    \newtheorem{lemma}[definition]{Lemma}
    \newtheorem{axiom}[definition]{Axiom}
    \newtheorem*{exercise}{Exercise}


\newcommand{\impl}{\rightarrow}
\newcommand{\eq}{\thicksim}
\newcommand{\quotient}{A/\thicksim}
\newcommand{\fun}[3]{#1\colon #2\rightarrow#3}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\restrict}{\upharpoonright}
\newcommand{\xor}{\oplus}

\title{Linear Algebra}
\author{for 93APM3 Applied Mathematics 3}
\date{AY25/26 Sem 2} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
This set of notes is based on \textit{Linear Algebra with Applications} by Steven J. Leon.
\newpage

\tableofcontents

\newpage

\section{Matrices}
A matrix is a rectangular array of numbers.
\subsection{Linear Systems}
\begin{definition}
A system of linear equations ("the System")
\begin{equation}\label{eq:1}
\begin{split}
a_1x+b_1y	&	=c_1\\
a_2x+b_2y	&	=c_2\\
\vdots\\
a_nx+b_ny	&	=c_n\\
\end{split}
\end{equation}
can be represented by matrices in the form
\[
AX=b
\]
where $A$ is the matrix of the coefficients, $X$ is the matrix of the directional vectors, and $b$ is a matrix of constants. Writing Eqs. (\ref{eq:1}) in this form we get
\begin{equation}
\left[\begin{array}{cc}
a_1		& b_1\\
a_2 		& b_2\\
\vdots 	& \vdots\\
a_n		& b_n
\end{array}\right]\cdot\left[\begin{array}{c}
x\\
y
\end{array}\right] = \left[\begin{array}{c}
c_1\\
c_2\\
\vdots\\
c_n
\end{array}\right].
\end{equation}
\begin{remark}
The matrix $A$ is the \textit{coefficient matrix} of the system.
\end{remark}
\end{definition}
\begin{definition}
In the \textbf{column form} this is represented as
\begin{equation}\label{eqn:col}
x\left[\begin{array}{c}
a_1\\
a_2\\
\vdots\\
a_n
\end{array}\right]+y\left[\begin{array}{c}
b_1\\
b_2\\
\vdots\\
b_n
\end{array}\right]=\left[\begin{array}{c}
c_1\\
c_2\\
\vdots\\
c_n
\end{array}\right]
\end{equation}
For $AX$, we say that $AX$ is a combination of the columns of $A$.
\end{definition}
\begin{remark}
Any linear equation in the form $AX=b$ can be solved, given that the matrix $A$ is a non-singular (or invertible) matrix.
\end{remark}
\begin{definition}
The \textbf{linear combination} refers to the combination of $x$ and $y$ which solves Eqs. (\ref{eqn:col}).
\end{definition}
\begin{definition}
The \textbf{augmented matrix} $M_ag$ is the matrix in the form
\[
\left[\begin{array}{c|c}
A	&	b
\end{array}\right]
\]
The system of linear equations can be solved by performing \textit{row operations} on the augmented matrix.
\end{definition}
\begin{definition}
The \textbf{elementary row operations} are
\begin{enumerate}
\item interchange two rows,
\item multiply a row by a non-zero real number, and
\item replace a row by its sum with a multiple of another row.
\end{enumerate}
Elementary row operations are performed on the matrix until the matrix is reduced to the \textit{row echelon form}.
\end{definition}

\begin{definition}
A matrix $A$ is in \textbf{row echelon form} if
\begin{enumerate}
\item the first non-zero entry in each non-zero row is 1,
\item row $k$ does not consist entirely of zeros, then the number of leading zeros in row $k+1$ is greater than that in row $k$, and
\item rows whose entries are all zero are below the rows having non-zero entries.
\end{enumerate}
For example,
\[
\left[\begin{array}{ccc}
1	&	2	&	3\\
0	&	0	&	1\\
0	&	0	&	0
\end{array}\right].
\]
\end{definition}

\begin{definition}
\textbf{Gaussian elimination} is the process of utilising the elementary row operations to bring a matrix into its row echelon form.
\end{definition}

\begin{definition}
$A$ is said to be in \textbf{reduced row echelon form} if
\begin{enumerate}
\item $A$ is in row echelon form, and
\item the first non-zero entry in each row is the only non-zero entry in its column.
\end{enumerate}
For example,
\[
\left[\begin{array}{cccc}
1	&	0	&	0	&	3\\
0	&	1	&	0	&	2\\
0	&	0	&	1	&	1
\end{array}\right].
\]
\end{definition}

\begin{definition}
\textbf{Gauss-Jordan reduction} is the process of utilising elementary row operations to bring a matrix to its reduced row echelon form.
\end{definition}

\begin{definition}
\textbf{Elementary matrices} $E$ perform the elimination procedures in Gaussian Elimination. For example,
\begin{equation}\label{eq:gausElimMatrix}
\begin{split}
\left[\begin{array}{ccc}
1	& 0	& 0\\
-3	& 1	& 0\\
0	& 0 & 1\\
\end{array}\right]\left[\begin{array}{ccc}
1	&	2	&	0\\
3	&	9	&	2\\
0	&	5	&	15\\
\end{array}\right]&=\left[\begin{array}{ccc}
1	&	2	&	0\\
0	&	1	&	2\\
0	&	5	&	15\\
\end{array}\right]\\
\left[\begin{array}{ccc}
1	&	0	&	0\\
0	&	1	&	0\\
0	&	-5	&	1\\
\end{array}\right]
\left[\begin{array}{ccc}
1	&	2	&	0\\
0	&	1	&	2\\
0	&	5	&	15\\
\end{array}\right]&=
\left[\begin{array}{ccc}
1	&	2	&	0\\
0	&	1	&	2\\
0	&	0	&	5\\
\end{array}\right]
\end{split}
\end{equation}
This can be expressed as
\[
B=E_2E_1A
\]
\end{definition}
\begin{definition}
The matrix $B$ is the \textbf{row equivalent} of $A$ when there exists a finite sequence of elementary matrices $E_n,E_{n-1},\cdots,E_1$ such that
\[
B=E_nE_{n-1}\cdots E_1A
\]
\end{definition}

\subsection{Matrix Multiplication}
It is given matrix $A$ (of size $n\times m$) and matrix $B$ (of size $m\times p$).
\begin{definition}
The size of the product matrix $AB$ is $n \times m$.
\end{definition}
\begin{definition}
For the matrix $AB$, the element at row $j$ and column $k$ is
\[
\sum_{x=1}^na_{jx}b_{xk}
\]
\end{definition}

\subsection{Inverses}
\begin{definition}
The \textbf{identity matrix} $I$ of a matrix $A$ is
\begin{equation}
\left[\begin{array}{cccc}
1 & 0 & \cdots & 0\\
0 & 1 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & 1\\
\end{array}\right].
\end{equation}
The size of $I$ is the same as that of $A$.
\end{definition}
\begin{definition}
If the \textbf{inverse} $A^{-1}$ of the matrix $A$ exists, then it is the matrix which fulfils
\begin{equation}
AA^{-1}=I
\end{equation}
where $I$ is the identity matrix.
\end{definition}
\begin{remark}
Such a matrix $A$ is a invertible/non-singular matrix.
\end{remark}
\begin{definition}
The inverse $B^-1$ \textbf{does not} exist when
\begin{equation*}
\begin{split}
\det B &= 0\text{, or}\\
B\textbf{X}&=0\text{ is solvable.}\\
\end{split}
\end{equation*}
\end{definition}
\begin{remark}
Such a matrix $B$ is a singular matrix. Its inverse does not exist.
\end{remark}
\begin{exercise}
We use \textbf{Gauss-Jordan elimination} to solve for $A^{-1}$ given $A$. Say we are given a matrix $A$, where
\[
A = \left[\begin{array}{cc}
1 & 3\\
2 & 7\\
\end{array}\right]
\]
To start, we combine $A$ and $I$ into a single augmented matrix
\[
\left[\begin{array}{cc|cc}
1 & 3 & 1 & 0\\
2 & 7 & 0 & 1\\
\end{array}\right]
\]
Perform elimination methods such that the matrix to the left of the vertical division is that of $I$; $A^{-1}$ would then be the matrix to the right of the vertical division.
\[
\left[\begin{array}{c|c}
I & A^{-1}\\
\end{array}\right]
\]
\end{exercise}
\begin{definition}
The inverse of the product of two matrices $AB$ is the matrix which fulfils
\begin{equation}
\begin{split}
I &= \left(AB\right)\left(B^{-1}A^{-1}\right)\\
\end{split}
\end{equation}
\end{definition}
\begin{definition}
The \textbf{transpose} $A^T$ of a matrix $A$ is the matrix where the rows and columns of $A$ have been swapped.
\[
A=\left[\begin{array}{ccc}
a & b & c\\
x & y & z\\
\end{array}\right] \to A^T=\left[\begin{array}{cc}
a & x\\
b & y\\
c & z\\
\end{array}\right]
\]
We can write that for an element of the matrix $A_{ij}$,
\[
A_{ij}=A_{ji}^T
\]
\end{definition}
\subsection{Elimination in the form A=LU}
\begin{discussion}
Given that no row exchanges occured, Eqs. (\ref{eq:gausElimMatrix}) was written in the form $EA=U$. It is preferable to write $EA=U$ in the form
\[
A = LU
\]
where $L=E^{-1}$.\\
In the matrix $L$, if no row exchanges occurred, the multipliers go directly into $L$.
\end{discussion}
\subsubsection{With row exchanges}
\begin{definition}
The permutation matrix $P$ are identity matrices with reordered rows. $P$ is used to execute row exchanges.
\end{definition}
\begin{remark}
For $P$, $P^{-1}=P^T$; for a $n\times n$ matrix, there are $n!$ permutations.
\end{remark}
\begin{discussion}Row exchanges become necessary when there is a 0 in the pivot spot. A row exchange with any row that has a non-zero digit in the pivot spot is necessary to perform elimination.
\end{discussion}
\begin{definition}
With row exchanges, $A=LU$ is written in the form
\[
PA=LU
\]
where $P$ are permutation matrices.
\end{definition}
\newpage


%%% VECTOR SPACES %%%
\section{Vector Spaces}
\subsection{Vector Spaces}
\begin{definition}
The \textbf{vector space} is a space of vectors. In a \textit{real} vector space, the basic operations
\begin{itemize}
	\item $v+w$,
	\item $cw$, and
	\item any such linear combination (i.e. $cv+dw$) of these operations.
\end{itemize}
remain closed (in the space).
\end{definition}
\begin{remark}
Real vector spaces must satisfy 8 conditions.
\begin{enumerate}
	\item $x+y=y+x$,
	\item $x+(y+z)=(x+y)+z$,
	\item there is a "zero vector" such that $x+0=x$,
	\item for each $x$ there is a unique vector $-x$ such that $x+(-x)=0$,
	\item $1\times x=x$,
	\item $\left(c_1c_2\right)x=c_1\left(c_2x\right)$,
	\item $c\left(x+y\right)=cx+cy$, and
	\item $\left(c_1+c_2\right)x=c_1x+c_2x$.
\end{enumerate}
These conditions are necessary to permit scalar multiplication and vector addition.
\end{remark}
\begin{exercise}
Some examples of vector spaces are:
\begin{itemize}
	\item $\mathbb{R}^2$ is the vector space of all 2-D real vectors.
	\item $\mathbb{R}^3$ is the vector space of all 3-D real vectors.
	\item In general, any space $\mathbb{R}^n$ is a vector space of all n-dimensional real vectors.
\end{itemize}
\end{exercise}

%%% SUBSPACES %%%
\subsection{Subspaces}
\begin{definition}
A \textbf{subspace} $S$ is a vector space within another vector space. $S$ must be a non-empty subset of $V$, and must satisfy:
\begin{enumerate}
\item $\alpha\textbf{x}\in S$ for $\textbf{x}\in S$ for any scalar $\alpha$, and
\item $\textbf{x}+\textbf{y}\in S$ whenever $\textbf{x},\textbf{y}\in S$.
\end{enumerate}
\end{definition}

\begin{exercise}
Some examples of vector subspaces in the vector space $\mathbb{R^2}$
\begin{itemize}
	\item all of $\mathbb{R^2}$,
	\item any line through $\left(0,0\right)$ in $\mathbb{R^2}$, and
	\item the zero vector alone.
\end{itemize}
\end{exercise}

\begin{definition}
The \textbf{Nullspace} $N\left(A\right)$ of the space $A$ is the set of all solutions of the homogeneous system $A\textbf{x}=\textbf{0}$.
\[
N\left(A\right) = \{ x\in A|A\textbf{x}=\textbf{0} \}
\]
\end{definition}

\begin{definition}
The \textbf{linear combination} of the vectors $v_1,v_2,\cdots,v_n$, which are vectors in a vector space $V$ is the sum of these vectors in the form
\begin{equation}\label{eq:linCombi}
\alpha_1v_1+\alpha_2v_2+\cdots \alpha_nv_n
\end{equation}
\end{definition}
\begin{definition}
The \textbf{span} is the set of all linear combinations of $v_1,v_2,\cdots,v_n$ and is denoted by $\text{Span}\{v_1,\cdots,v_n\}$.\\
\\
The set $\{v_1,\cdots v_n\}$ is a \textbf{spanning set} for $V$ is and only if every vector in $V$ can be written as a linear combination of $v_1,v_2,\cdots v_n$.
\end{definition}

\subsection{Linear Independence}
\begin{definition}
The vectors $v_1,v_2,\cdots,v_n$ in a vector space $V$ are \textbf{linearly independent} if
\[
c_1v_1+c_2v_2+\cdots+c_nv_n=0
\]
implies that $c_1,c_2,\cdots,c_n$ must be 0.
\end{definition}
\begin{definition}
The vectors $v_1,v_2,\cdots,v_n$ in a vector space $V$ are \textbf{linearly dependent} if there exists scalars such that
\[
c_1v_1+c_2v_2+\cdots+c_nv_n=0
\]
\end{definition}
\subsection{Basis \& Dimension}
\begin{definition}
The \textbf{basis} of a vector is formed by the set of vectors $\{v_1\cdots v_n\}$ if and only if
\begin{enumerate}
\item $v_1,\cdots,v_n$ are linearly independent, and
\item $v_1,\cdots,v_n$ set spans $V$.
\end{enumerate}
A vector space can have one or more vector bases.
\end{definition}
\begin{definition}
If the basis of $V$ has $n$ elements, then $V$ is said to be of $n$-dimensions. If there exists a finite set of vectors that span $V$, then $V$ is said to be finite dimensional; $V$ is otherwise infinitely dimensional.
\end{definition}

\subsection{Row and Column Space}
\begin{definition}
The \textbf{row space} $R\left(A\right)$ is the subspace of $\mathbb{R}^n$ which contains all the linear combinations of the rows of the matrix $A$.
\[
\text{Row}\left(A\right)=R\left(A\right)=\text{Lin}\{r_1,r_2,\cdots,r_n\}
\]
where $r_1,r_2,\cdots,r_n$ are the rows of $A$.
\end{definition}
\begin{definition}
The \textbf{rank} of $A$
\[
\text{rank}\left(A\right)
\]
is the dimension of the row space of $A$.
\end{definition}

\begin{definition}
The \textbf{column space} $C\left(A\right)$ is the subspace of $\mathbb{R}^n$ which contains all the linear combinations of the columns of the matrix $A$.
\[
\text{Col}\left(A\right)=C\left(A\right)=\text{Lin}\{c_1,c_2,\cdots,c_n\}
\]
where $c_1,c_2,\cdots,c_n$ are the columns of $A$.
\end{definition}

\subsection{Eigenvalues and Eigenvectors}
Given that $A$ is a square matrix of size $n\times n$. If there exists a \textbf{non-zero vector} $x$ such that 
\[
Ax=\lambda x,
\]
then
\begin{definition}
the scalar $\lambda$ is a \textbf{eigenvalue} or \textbf{characteristic value} of $A$, and
\end{definition}
\begin{definition}
the vector $x$ is an \textbf{eigenvector} or \textbf{characteristic vector} of $\lambda$.
\end{definition}
\begin{discussion}
The eigenvalues $\lambda$ and eigenvalues $x$ of $A$ can be determined by
\begin{enumerate}
\item solving the characteristic equation $\text{det}\left(A-\lambda I\right)=0$ which determines the eigenvalues $\lambda$, and then
\item for each of the eigenvalues $\lambda$, solve the homogenous system $\left(A-\lambda I\right)v=0$ which determines the corresponding eigenvector(s).  
\end{enumerate}
\end{discussion}
\begin{theorem}
The product of the eigenvalues $\lambda_n$ of $A$ gives the determinant of $A$.
\[
\Pi^n_{i=1}\left(\lambda_n\right)=\text{det}\left(A\right)
\]
\end{theorem}
\begin{theorem}
For the eigenvectors $\lambda_n$ and $\lambda_m$ to be linearly independent, they must correspond to different eigenvalues $x_n$ and $x_m$.
\end{theorem}
\subsubsection{Similar Matrices}
\begin{definition}
The matrices $A$ and $B$ are said to be \textbf{similar matrices} if there exists a non-singular matrix $S$ such that
\[
B=S^{-1}AS
\]
\end{definition}
\begin{theorem}
If two matrices $A$ and $B$ are similar, then $A$ and $B$ have the same characteristic equation and hence the same eigenvalues.
\end{theorem}

\subsection{Diagonalisation}
\begin{definition}
The $n\times n$ matrix $A$ is said to be \textbf{diagonalisable} if there exists a non-singular matrix $X$ and diagonal matrix $D$ such that
\[
X^-1AX=D
\]
then $X$ diagonalises $A$.
\end{definition}
\begin{remark}
For $A$ to be diagonalisable, $A$ must have $n$ linearly independent eigenvectors, which must correspond to distinct eigenvalues. If $A$ is diagonalisable, then
\begin{enumerate}
\item the column vectors of $X$ correspond to the eigenvectors $x$ of $A$,
\[
X=\left[\begin{array}{cccc}
x_1	&	x_2	&	\cdots &	x_n
\end{array}\right]
\]
\item the diagonal elements of $D$ correspond to the eigenvalues of $A$.
\[
D=\left[\begin{array}{cccc}
\lambda_1	&			&			&\\
			&\lambda_2	&			&\\
			&			&\ddots		&\\
			&			&			&\lambda_n
\end{array}\right]
\]
\end{enumerate}
\textbf{The square matrix is diagonalisable if it has $n$ linearly independent eigenvalues. It does not need to have $n$ distinct eigenvectors.}
\end{remark}
\begin{exercise} \textbf{Powers of matrices}\\
If $A$ is diagonalizable, then $A$ can be factored into the form $XDX^{-1}$. Therefore,
\[
A^k=XD^kX^{-1}=X\left[\begin{array}{cccc}
\left(\lambda_1\right)^k	&							&			&\\
							&\left(\lambda_2\right)^k	&			&\\
							&							&\ddots		&\\
							&							&			&\left(\lambda_n\right)^k
\end{array}\right]X^{-1}
\]

\end{exercise}

\subsection{Systems of Linear Differential Equations}
\begin{definition}
A system of differential equations (of the first degree)
\begin{equation}
\begin{split}
{y'}_1	&=	a_{11}y_1+a_{12}y_2+\cdots+a_{1n}y_n\\
{y'}_2	&=	a_{21}y_1+a_{22}y_2+\cdots+a_{2n}y_n\\
\vdots\\
{y'}_n	&=	a_{n1}y_1+a_{n2}y_2+\cdots+a_{nn}y_n
\end{split}
\end{equation}
and can be represented in the matrix form
\begin{equation}
\textbf{y}'=A\textbf{y}
\end{equation}
\end{definition}
\begin{remark}
Any equation in the form
\[
\frac{dy}{dx}=kx
\]
can be solved with the general solution
\[
x=Ae^{kt},A\in\mathbb{R}
\]
Thus, for the case $n>1$,
\[
\textbf{Y}=\left[\begin{array}{c}
x_1e^{\lambda t}\\
x_2e^{\lambda t}\\
\vdots\\
x_ne^{\lambda t}
\end{array}\right]
=e^{\lambda t}\textbf{x}
\]
\end{remark}
\begin{definition}
The \textbf{initial value problem} is any problem of the form
\[
\begin{split}
\textbf{Y}'&=A\textbf{Y}\\
\textbf{Y}\left(0\right)&=\textbf{Y}_0
\end{split}.
\]
Any such problem in the form will have a unique solution.
\end{definition}












\end{document}
